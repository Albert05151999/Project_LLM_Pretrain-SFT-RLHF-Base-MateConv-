{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751962fd-529e-4d29-a4ea-899fe48a59b0",
   "metadata": {},
   "source": [
    "# MateConv mini Inference and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e764c5d-19d7-4379-8e24-fd9b8457b02b",
   "metadata": {},
   "source": [
    "## 1. 两种微调架构的模型导入--FFN和MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208b734c-cee3-4fe5-8343-ff675595db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/MateConv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from model.model import Transformer  # 确保路径正确\n",
    "from model.LMConfig import LMConfig\n",
    "from model.LMConfig_FFN import LMConfig_FFN   # 导入 LMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a19b89f-b998-4eee-a316-16e891cc94ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义BOS和EOS标记\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a08ecd-14f6-4905-851d-b8794d4b6d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的tokenizer词表大小: 6400\n"
     ]
    }
   ],
   "source": [
    "# 加载训练好的分词器路径\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/mateconv_tokenizer', use_fast=False)\n",
    "print(f'加载的tokenizer词表大小: {len(tokenizer)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e123fa-65ed-446d-b851-ad7a3e068b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建配置对象\n",
    "lm_config_moe = LMConfig()\n",
    "lm_config_ffn = LMConfig_FFN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0240abb-5a5b-4333-939f-b932503e152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 Transformer 模型\n",
    "model_moe = Transformer(lm_config_moe)\n",
    "model_ffn = Transformer(lm_config_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a744ab6-f2e6-4c82-b8d9-e6d4260dee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4d3851-b1c5-4d89-a0a5-0b2371a9a7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641a636e-4c9c-43db-b08e-1418d1c49d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(6400, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): MOEFeedForward(\n",
      "        (experts): ModuleList(\n",
      "          (0-3): 4 x FeedForward(\n",
      "            (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "            (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "            (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (gate): MoEGate()\n",
      "        (shared_experts): FeedForward(\n",
      "          (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "          (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "          (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_moe.to(device)\n",
    "\n",
    "# 检查模型结构和参数\n",
    "print(model_moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6054964e-fd04-44f7-846d-74313ecb16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(6400, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
      "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
      "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ffn.to(device)\n",
    "print(model_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e335e0ed-47e2-4768-92bc-74e3964eff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_state_dict(ckpt_obj):\n",
    "    # 常见保存格式：直接为 state_dict，或包含 'model_state' / 'state_dict' 键的 dict\n",
    "    if isinstance(ckpt_obj, dict):\n",
    "        if 'model_state' in ckpt_obj:\n",
    "            return ckpt_obj['model_state']\n",
    "        if 'state_dict' in ckpt_obj:\n",
    "            return ckpt_obj['state_dict']\n",
    "        # 有时候保存时把 state_dict 放在第一个 value 中\n",
    "        # 尝试从 values 中找到第一个看起来像 state_dict 的对象\n",
    "        for v in ckpt_obj.values():\n",
    "            if isinstance(v, dict) and all(isinstance(x, torch.Tensor) for x in v.values()):\n",
    "                return v\n",
    "        # 否则，可能本身就是 state_dict（键->tensor）\n",
    "        if all(isinstance(x, torch.Tensor) for x in ckpt_obj.values()):\n",
    "            return ckpt_obj\n",
    "        # 不能识别\n",
    "        return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def strip_prefix(state_dict, prefix):\n",
    "    new_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix):\n",
    "            new_state[k[len(prefix):]] = v\n",
    "        else:\n",
    "            new_state[k] = v\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccbd6c8-1f77-4275-b3db-52da7a9764e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_state_dict result: <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(6400, 512)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载ffn微调模型\n",
    "# 加载模型权重（支持 raw state_dict 或训练时保存的 checkpoint dict）\n",
    "ckpt_path = 'out/full_sft_512.pth'  # 改为你的 checkpoint 路径，或 'out/pretrain_512.pth'\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = find_state_dict(ckpt)\n",
    "if state_dict is None:\n",
    "    raise RuntimeError(f\"无法从 checkpoint 中识别出 state_dict，checkpoint keys: {list(ckpt.keys()) if isinstance(ckpt, dict) else type(ckpt)}\")\n",
    "# 移除常见的分布式/包装前缀\n",
    "if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "    state_dict = strip_prefix(state_dict, 'module.')\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "    state_dict = strip_prefix(state_dict, '_orig_mod.')\n",
    "\n",
    "# 加载到模型（使用 strict=False 以便更好定位缺失/多余键）\n",
    "res = model_ffn.load_state_dict(state_dict, strict=False)\n",
    "print('load_state_dict result:', res)\n",
    "model_ffn.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc57d52d-448d-46c3-9509-b64704fbac9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_state_dict result: <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(6400, 512)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (feed_forward): MOEFeedForward(\n",
       "        (experts): ModuleList(\n",
       "          (0-3): 4 x FeedForward(\n",
       "            (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "            (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "            (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (gate): MoEGate()\n",
       "        (shared_experts): FeedForward(\n",
       "          (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "          (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "          (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载moe模型\n",
    "# 加载模型权重（支持 raw state_dict 或训练时保存的 checkpoint dict）\n",
    "ckpt_path_moe = 'out/full_sft_512_moe.pth'  # 改为你的 checkpoint 路径，或 'out/pretrain_512.pth'\n",
    "ckpt_moe = torch.load(ckpt_path_moe, map_location=device)\n",
    "state_dict_moe = find_state_dict(ckpt_moe)\n",
    "if state_dict_moe is None:\n",
    "    raise RuntimeError(f\"无法从 checkpoint 中识别出 state_dict，checkpoint keys: {list(ckpt.keys()) if isinstance(ckpt, dict) else type(ckpt)}\")\n",
    "# 移除常见的分布式/包装前缀\n",
    "if any(k.startswith('module.') for k in state_dict_moe.keys()):\n",
    "    state_dict_moe = strip_prefix(state_dict_moe, 'module.')\n",
    "if any(k.startswith('_orig_mod.') for k in state_dict_moe.keys()):\n",
    "    state_dict_moe = strip_prefix(state_dict_moe, '_orig_mod.')\n",
    "\n",
    "# 加载到模型（使用 strict=False 以便更好定位缺失/多余键）\n",
    "res = model_moe.load_state_dict(state_dict_moe, strict=False)\n",
    "print('load_state_dict result:', res)\n",
    "model_moe.eval()  # 切换到评估模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4eff9-c515-4720-981d-ff40991b5179",
   "metadata": {},
   "source": [
    "## 2. BenchMark评价模型（Token-F1 / ROUGE-L）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d523b6f2-9125-4fbf-ab49-446cd1d1a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/MateConv/lib/python3.10/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Part 1. 环境变量与函数定义\n",
    "import os, re, io, random, contextlib, ast\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import jieba\n",
    "\n",
    "# 评测尽量可复现：固定随机种子\n",
    "torch.manual_seed(2025)\n",
    "random.seed(2025)\n",
    "np.random.seed(2025)\n",
    "\n",
    "# —— 上下文：临时关闭确定性（避免 cuBLAS 报错），结束后恢复原状态\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def temporarily_disable_determinism():\n",
    "    prev = torch.are_deterministic_algorithms_enabled()\n",
    "    if prev:\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if prev:\n",
    "            torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# ===== 1) 数据导入 =====\n",
    "DATA_PATH = \"./dataset/sft_data_mixed_single.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"找不到数据文件：{DATA_PATH}\"\n",
    "df1 = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# ===== 2) 解析 & 构造 prompt =====\n",
    "def parse_history(raw):\n",
    "    if raw is None:\n",
    "        return []\n",
    "    if isinstance(raw, list):\n",
    "        return raw\n",
    "    s = str(raw).strip()\n",
    "    if not s or s == \"[]\" or s.lower() == \"nan\":\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def build_messages(q, history):\n",
    "    msgs = []\n",
    "    for h in history:\n",
    "        if not isinstance(h, (list, tuple)) or len(h) < 2:\n",
    "            continue\n",
    "        msgs.append({\"role\": \"user\", \"content\": str(h[0])})\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": str(h[1])})\n",
    "    msgs.append({\"role\": \"user\", \"content\": str(q)})\n",
    "    return msgs\n",
    "\n",
    "def safe_postprocess(text: str):\n",
    "    return str(text).strip()\n",
    "\n",
    "# ===== 3) 生成（评测默认“确定化”，预览可覆盖）=====\n",
    "def pick_eos_id(tokenizer):\n",
    "    for tok in [\"<|eot_id|>\", \"<|end|>\", \"</s>\", \"<eos>\"]:\n",
    "        try:\n",
    "            tid = tokenizer.convert_tokens_to_ids(tok)\n",
    "            if isinstance(tid, int) and tid >= 0:\n",
    "                return tid\n",
    "        except Exception:\n",
    "            pass\n",
    "    return getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_one_with_model(\n",
    "    q, history, tokenizer, model, device,\n",
    "    max_new_tokens=256, temperature=0.0, top_k=0, rp=1.0, debug=False\n",
    "):\n",
    "    messages = build_messages(q, history)\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc.input_ids.to(device)\n",
    "    eos_id = pick_eos_id(tokenizer)\n",
    "\n",
    "    out_ids = next(model.generate(\n",
    "        idx=input_ids,\n",
    "        eos=eos_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,   # 评测：0.0；预览可覆盖\n",
    "        top_k=top_k,               # 评测：0；   预览可覆盖\n",
    "        rp=rp,                     # 评测：1.0； 预览可覆盖\n",
    "        stream=False\n",
    "    ))\n",
    "\n",
    "    out_ids = out_ids[0]\n",
    "    gen_part = out_ids[input_ids.shape[1]:]\n",
    "    text_raw_gen = tokenizer.decode(gen_part, skip_special_tokens=False)\n",
    "    pred = safe_postprocess(text_raw_gen)\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\n[DEBUG] prompt_len:\", int(input_ids.shape[1]),\n",
    "              \" total_len:\", int(out_ids.shape[0]),\n",
    "              \" gen_len:\", int(gen_part.shape[0]),\n",
    "              \" eos_id:\", eos_id)\n",
    "        print(\"[DEBUG] RAW_GEN(head):\", repr(text_raw_gen[:200]))\n",
    "        print(\"[DEBUG] PRED(head):   \", repr(pred[:200]))\n",
    "    return pred\n",
    "\n",
    "# ===== 4) 指标（中文 Token-F1 / ROUGE-L）=====\n",
    "CN_PUNC = \"，。、“”《》；：？！【】（）—…·‘’\"\n",
    "EN_PUNC = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "PUNC_TABLE = str.maketrans(\"\", \"\", EN_PUNC + CN_PUNC)\n",
    "\n",
    "def _normalize_text_for_zh(s: str):\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    s = s.translate(PUNC_TABLE)\n",
    "    return s\n",
    "\n",
    "def _to_word_tokens(s: str):\n",
    "    s = _normalize_text_for_zh(s)\n",
    "    return [w for w in jieba.lcut(s) if w]\n",
    "\n",
    "def metric_token_f1(pred, ref):\n",
    "    p_tokens = _to_word_tokens(pred)\n",
    "    r_tokens = _to_word_tokens(ref)\n",
    "    if not p_tokens and not r_tokens:\n",
    "        return 1.0\n",
    "    if not p_tokens or not r_tokens:\n",
    "        return 0.0\n",
    "    cp, cr = Counter(p_tokens), Counter(r_tokens)\n",
    "    overlap = sum((cp & cr).values())\n",
    "    prec = overlap / max(1, len(p_tokens))\n",
    "    rec  = overlap / max(1, len(r_tokens))\n",
    "    if prec == 0 and rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def _lcs_len(a_tokens, b_tokens):\n",
    "    n, m = len(a_tokens), len(b_tokens)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(n):\n",
    "        ai = a_tokens[i]\n",
    "        row = dp[i]\n",
    "        row_next = dp[i+1]\n",
    "        for j in range(m):\n",
    "            if ai == b_tokens[j]:\n",
    "                row_next[j+1] = row[j] + 1\n",
    "            else:\n",
    "                row_next[j+1] = max(row[j+1], row_next[j])\n",
    "    return dp[n][m]\n",
    "\n",
    "def metric_rougeL(pred, ref):\n",
    "    p = _to_word_tokens(pred)\n",
    "    r = _to_word_tokens(ref)\n",
    "    if not p or not r:\n",
    "        return 0.0\n",
    "    lcs = _lcs_len(p, r)\n",
    "    prec = lcs / len(p)\n",
    "    rec  = lcs / len(r)\n",
    "    if prec == 0 and rec == 0:\n",
    "        return 0.0\n",
    "    return (2 * prec * rec) / (prec + rec)\n",
    "\n",
    "# ===== 5) 模型概览 =====\n",
    "def n_params(m): \n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "def summarize_model(name, m):\n",
    "    print(f\"\\n[{name}] 参数量: {n_params(m):,}\")\n",
    "    moe_like = [k for k,_ in m.named_modules() if (\"moe\" in k.lower() or \"expert\" in k.lower())]\n",
    "    print(f\"[{name}] 模块名含 'moe/experts' 的组件数: {len(moe_like)}\")\n",
    "    if len(moe_like) > 0:\n",
    "        print(\"  示例：\", moe_like[:5])\n",
    "\n",
    "# ===== 6) 两模型并排基准（可打印每样本；批量时会被静默）=====\n",
    "def benchmark_pair(df, tokenizer, model_moe, model_ffn, device,\n",
    "                   n_samples=3, save_path=None):\n",
    "    assert len(df) >= n_samples, f\"数据不足 {n_samples} 条\"\n",
    "    data = df.head(n_samples).copy()\n",
    "\n",
    "    summarize_model(\"MOE\", model_moe)\n",
    "    summarize_model(\"FFN\", model_ffn)\n",
    "\n",
    "    rows = []\n",
    "    agg = {\"moe_F1\": [], \"moe_RL\": [], \"ffn_F1\": [], \"ffn_RL\": []}\n",
    "\n",
    "    for i, (_, row) in enumerate(data.iterrows(), 1):\n",
    "        q = str(row.get(\"q\", \"\")).strip()\n",
    "        ref = str(row.get(\"a\", \"\")).strip()\n",
    "        history = parse_history(row.get(\"history\", \"[]\"))\n",
    "\n",
    "        pred_moe = infer_one_with_model(q, history, tokenizer, model_moe, device,\n",
    "                                        max_new_tokens=256, temperature=0.0, top_k=0, rp=1.0)\n",
    "        pred_ffn = infer_one_with_model(q, history, tokenizer, model_ffn, device,\n",
    "                                        max_new_tokens=256, temperature=0.0, top_k=0, rp=1.0)\n",
    "\n",
    "        moe_f1 = metric_token_f1(pred_moe, ref);     ffn_f1 = metric_token_f1(pred_ffn, ref)\n",
    "        moe_rl = metric_rougeL(pred_moe, ref);       ffn_rl = metric_rougeL(pred_ffn, ref)\n",
    "\n",
    "        agg[\"moe_F1\"].append(moe_f1); agg[\"moe_RL\"].append(moe_rl)\n",
    "        agg[\"ffn_F1\"].append(ffn_f1); agg[\"ffn_RL\"].append(ffn_rl)\n",
    "\n",
    "        rows.append({\n",
    "            \"q\": q, \"ref\": ref,\n",
    "            \"pred_moe\": pred_moe, \"pred_ffn\": pred_ffn,\n",
    "            \"F1_moe\": moe_f1, \"ROUGE-L_moe\": moe_rl,\n",
    "            \"F1_ffn\": ffn_f1, \"ROUGE-L_ffn\": ffn_rl,\n",
    "        })\n",
    "\n",
    "        # 单样本打印（批量跑时会被外层静默掉）\n",
    "        print(f\"\\n--- Sample #{i} ---\")\n",
    "        print(\"Q  :\", q[:400])\n",
    "        print(\"REF:\", ref[:400])\n",
    "        print(\"[MOE]:\", pred_moe[:400])\n",
    "        print(\"[FFN]:\", pred_ffn[:400])\n",
    "        print(f\"F1/RL (MOE): {moe_f1:.3f}/{moe_rl:.3f}\")\n",
    "        print(f\"F1/RL (FFN): {ffn_f1:.3f}/{ffn_rl:.3f}\")\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    def _avg(xs): \n",
    "        return float(sum(xs)/max(1,len(xs)))\n",
    "\n",
    "    print(\"\\n=== Benchmark 汇总(前{}条) ===\".format(n_samples))\n",
    "    print(\"MOE : F1={:.3f}  ROUGE-L={:.3f}\".format(_avg(agg['moe_F1']), _avg(agg['moe_RL'])))\n",
    "    print(\"FFN : F1={:.3f}  ROUGE-L={:.3f}\".format(_avg(agg['ffn_F1']), _avg(agg['ffn_RL'])))\n",
    "\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        out.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"\\nSaved to: {save_path}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ===== 7) 采样与 CI 工具 =====\n",
    "def sample_df(df, n=500, seed=2025):\n",
    "    if \"task_type\" in df.columns:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        parts = []\n",
    "        for t, grp in df.groupby(\"task_type\"):\n",
    "            k = max(1, int(round(len(grp) / len(df) * n)))\n",
    "            parts.append(grp.sample(n=min(k, len(grp)), random_state=int(rng.integers(1, 1_000_000_000))))\n",
    "        out = pd.concat(parts, ignore_index=True)\n",
    "        if len(out) > n:\n",
    "            out = out.sample(n=n, random_state=seed)\n",
    "        return out.reset_index(drop=True)\n",
    "    if len(df) <= n:\n",
    "        return df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return df.sample(n=n, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "def mean_ci(values, B=1000, alpha=0.05, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    vals = np.asarray(values, dtype=float)\n",
    "    if len(vals) == 0:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    boots = []\n",
    "    for _ in range(B):\n",
    "        s = rng.choice(vals, size=len(vals), replace=True)\n",
    "        boots.append(s.mean())\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return float(vals.mean()), float(lo), float(hi)\n",
    "\n",
    "def paired_diff_ci(values_a, values_b, B=1000, alpha=0.05, seed=321):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(values_a, dtype=float)\n",
    "    b = np.asarray(values_b, dtype=float)\n",
    "    assert len(a) == len(b) and len(a) > 0\n",
    "    d = a - b\n",
    "    boots = []\n",
    "    for _ in range(B):\n",
    "        idx = rng.integers(0, len(d), size=len(d))\n",
    "        boots.append(d[idx].mean())\n",
    "    lo, hi = np.percentile(boots, [100*alpha/2, 100*(1-alpha/2)])\n",
    "    return float(d.mean()), float(lo), float(hi)\n",
    "\n",
    "# ===== 8) 预览（带温度采样）=====\n",
    "def preview_random_samples(\n",
    "    df, tokenizer, model_moe, model_ffn, device,\n",
    "    n=3, seed=2025, max_new_tokens=256,\n",
    "    temperature=0.7, top_k=50, rp=1.15\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sub = df if len(df) <= n else df.sample(n=n, random_state=int(rng.integers(1, 1_000_000_000)))\n",
    "\n",
    "    # 预览：临时关闭确定性（避免 cuBLAS 限制报错）\n",
    "    with temporarily_disable_determinism():\n",
    "        print(f\"\\n[Preview] n={len(sub)}  (temp={temperature}, top_k={top_k}, rp={rp})\")\n",
    "        for i, (_, row) in enumerate(sub.iterrows(), 1):\n",
    "            q = str(row.get(\"q\", \"\")).strip()\n",
    "            ref = str(row.get(\"a\", \"\")).strip()\n",
    "            history = parse_history(row.get(\"history\", \"[]\"))\n",
    "\n",
    "            moe_pred = infer_one_with_model(\n",
    "                q, history, tokenizer, model_moe, device,\n",
    "                max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, rp=rp\n",
    "            )\n",
    "            ffn_pred = infer_one_with_model(\n",
    "                q, history, tokenizer, model_ffn, device,\n",
    "                max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k, rp=rp\n",
    "            )\n",
    "\n",
    "            print(f\"\\n--- Preview #{i} ---\")\n",
    "            print(\"Q  :\", q[:400])\n",
    "            if ref:\n",
    "                print(\"REF:\", ref[:400])\n",
    "            print(\"[MOE]:\", moe_pred[:500])\n",
    "            print(\"[FFN]:\", ffn_pred[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f2fb7f-993e-4e23-9699-20126297b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Preview] n=3  (temp=0.7, top_k=50, rp=1.15)\n",
      "\n",
      "--- Preview #1 ---\n",
      "Q  : 上联：剪一缕春风裱画\n",
      "下联：\n",
      "REF: 拈几桢梅影酬朋\n",
      "[MOE]: 曲歌三更，绣阁秋光万里香</s>\n",
      "</s>\n",
      "最爱桃花映水晶湖。</s>\n",
      "烟笼柳絮飘零雨，明朝停杯话别离</s>\n",
      "</s>\n",
      "谁为倚栏愁怀客是此间，无人知否？</s>\n",
      "绝胜杏花楼外笑语声，一笑泪湿相思衣冠</s>\n",
      "悄然逝去，只愿今宵醉眼看山行。</s>\n",
      "垂老大陆孤魂渺渺渺。</s>\n",
      "酒已成尸魂黍离死不再回，谁道人生如梦中来急！诗成有意同溯远方，天涯海角犹未归。唐虞之时，亦自得其乐\n",
      "[FFN]: 春光正好</s>\n",
      "</s>\n",
      "此来新年喜郎吝？</s>\n",
      "看春色满庭院里花，迎接清明几时休。</s>\n",
      "梅花易逢红娘来，花开不待艳阳天</s>\n",
      "多少年前爱恨情仇难消受\n",
      "红颜不负春事未卜君</s>\n",
      "月色常在日暮余华</s>\n",
      "风雨同归身俗者成病也，时有阴沉忧愁心事空\n",
      "忙碌又遭遇命运多舛重，今宵无眠中分割，世事苟且悬匿\n",
      "万物皆与物是人间非所趋\n",
      "梦醒来何处可数莺声鸣，夜半明月我独影寂寥。</s>\n",
      "小桃花\n",
      "\n",
      "--- Preview #2 ---\n",
      "Q  : 1. In the field of quantum mechanics, the Heisenberg uncertainty principle states that it is impossible to simultaneously determine both the position and momentum of a particle with precise accuracy.\n",
      "REF: 1. What is the Heisenberg uncertainty principle?\n",
      "[MOE]: ? If they were very easy to move forward after healing with our own strengths and weaknesses.</s>\n",
      "are you will have a sense of understanding the process or honesty in order that works well as the resolution of your specific use case. It would be best to try out different ways to ensure that making them better!</s>\n",
      "Hope the speaker may find helpful information on what we can do so. Both experiences offer food-supported situations and their new experiences. Is there anything else I can assist you \n",
      "[FFN]: LL AB VIP OverShades.</s>\n",
      "Three combines the Market Synivers cuts for the PSTRAN and BBC: \"The Market Synonyms of Market Connection\"</s>\n",
      "Non-Synilar Figm起来. It was not explicitly defined as an actually not a clear outcome.</s>\n",
      "In this example. The article should take your use of Market Synonyms prohibits to create more information about\n",
      "\n",
      "--- Preview #3 ---\n",
      "Q  : 根据以下条件筛选并排序我的收件箱中的电子邮件。请按重要性升序排列并显示前五封邮件。\n",
      "- 收件人：我\n",
      "- 主题：工作\n",
      "- 时间范围：上周\n",
      "REF: 根据您的条件，可以按照以下步骤筛选和排序您的电子邮件： \n",
      "1. 在收件箱中，应用过滤器，将所有收件人为“我”、主题包含“工作”且时间范围是上周的电子邮件筛选出来。 \n",
      "2. 将筛选出的电子邮件按照重要性升序排列。 \n",
      "3. 查看前五封电子邮件，即为符合条件的前五封邮件。\n",
      "[MOE]: 使用Spotify内置的搜索功能来找到最适合您的电子邮件。</s>\n",
      "（注：此为AI模型输出的插入提示）</s>\n",
      "\n",
      "1. 在邮箱页面上输入收件人的姓名、联系方式和地址等个人信息；\n",
      "2. 使用自然语言生成器自动生成回复，例如选择喜欢的话题或与主体进行联想。\n",
      "3. 将收件人名称的文本导出为PDF文档或图片，方便阅读和编辑。\n",
      "4. 根据\n",
      "[FFN]: 是首要任务。如果您有任何具体的意见或建议，可以随时告诉我。</s>\n",
      "如果问题不清晰，您可以尝试使用一些逻辑来确定最佳答案。例如，如果您希望这个邮件是在今天上午的会议上提到的，那么第二次会议可能更适合您。如果主题无法回答，则我们可以考虑使用其他时间和地点，以提供更多细节或意见。</s>\n",
      "结果是，邮件应该包含以下信息：\n",
      "1. 您准备好了吗？\n",
      "2. 在第五份日期之前，您需要做哪些事情？\n",
      "3.\n"
     ]
    }
   ],
   "source": [
    "# Part 2. 浏览/预览（看 3 条，带温度）\n",
    "preview_random_samples(\n",
    "    df1, tokenizer, model_moe, model_ffn, device,\n",
    "    n=3, seed=2025, temperature=0.7, top_k=50, rp=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1c5c6-4ab2-40b5-a3e5-a0d31199b575",
   "metadata": {},
   "source": [
    "随机抽样 500 条做评测，并计算 Token-F1 / ROUGE-L 的 95% 置信区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c073956-bdf6-4228-aa56-e63e81a949a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.611 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 1/5 (0-99) ===\n",
      "MOE  F1 : 0.141\n",
      "FFN  F1 : 0.143\n",
      "MOE  RL : 0.110\n",
      "FFN  RL : 0.110\n",
      "\n",
      "=== Batch 2/5 (100-199) ===\n",
      "MOE  F1 : 0.148\n",
      "FFN  F1 : 0.133\n",
      "MOE  RL : 0.121\n",
      "FFN  RL : 0.106\n",
      "\n",
      "=== Batch 3/5 (200-299) ===\n",
      "MOE  F1 : 0.170\n",
      "FFN  F1 : 0.158\n",
      "MOE  RL : 0.136\n",
      "FFN  RL : 0.125\n",
      "\n",
      "=== Batch 4/5 (300-399) ===\n",
      "MOE  F1 : 0.135\n",
      "FFN  F1 : 0.122\n",
      "MOE  RL : 0.104\n",
      "FFN  RL : 0.095\n",
      "\n",
      "=== Batch 5/5 (400-499) ===\n",
      "MOE  F1 : 0.180\n",
      "FFN  F1 : 0.173\n",
      "MOE  RL : 0.137\n",
      "FFN  RL : 0.137\n",
      "\n",
      "=== 95% CI (bootstrap, n=500) ===\n",
      "MOE  F1 : 0.155 [0.142, 0.168]\n",
      "FFN  F1 : 0.146 [0.134, 0.157]\n",
      "MOE  RL : 0.122 [0.112, 0.131]\n",
      "FFN  RL : 0.115 [0.106, 0.123]\n",
      "\n",
      "=== Paired diff (MOE − FFN) 95% CI ===\n",
      "ΔF1  : 0.009 [0.001, 0.018]\n",
      "ΔRL  : 0.007 [0.000, 0.014]\n",
      "→ F1 差异在 95% CI 下显著。\n",
      "→ ROUGE-L 差异在 95% CI 下显著。\n"
     ]
    }
   ],
   "source": [
    "# Part 3. 正式运行（500 样本，分批汇总 + 最终 95%CI）\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "TOTAL_N    = 500\n",
    "SEED       = 2025\n",
    "\n",
    "# 抽样一次并打乱\n",
    "pool = sample_df(df1, n=min(TOTAL_N, len(df1)), seed=SEED).reset_index(drop=True)\n",
    "\n",
    "all_chunks = []\n",
    "num_batches = (len(pool) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for b in range(num_batches):\n",
    "    start = b * BATCH_SIZE\n",
    "    end   = min((b + 1) * BATCH_SIZE, len(pool))\n",
    "    if end <= start:\n",
    "        break\n",
    "    chunk = pool.iloc[start:end].reset_index(drop=True)\n",
    "\n",
    "    # 静默运行 benchmark_pair + 临时关闭确定性（避免 cuBLAS 报错）\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        with temporarily_disable_determinism():\n",
    "            out_chunk = benchmark_pair(\n",
    "                df=chunk,\n",
    "                tokenizer=tokenizer,\n",
    "                model_moe=model_moe,\n",
    "                model_ffn=model_ffn,\n",
    "                device=device,\n",
    "                n_samples=len(chunk),\n",
    "                save_path=None\n",
    "            )\n",
    "\n",
    "    # 批次均值\n",
    "    moe_f1_mean = float(np.mean(out_chunk[\"F1_moe\"]))\n",
    "    ffn_f1_mean = float(np.mean(out_chunk[\"F1_ffn\"]))\n",
    "    moe_rl_mean = float(np.mean(out_chunk[\"ROUGE-L_moe\"]))\n",
    "    ffn_rl_mean = float(np.mean(out_chunk[\"ROUGE-L_ffn\"]))\n",
    "\n",
    "    print(f\"\\n=== Batch {b+1}/{num_batches} ({start}-{end-1}) ===\")\n",
    "    print(f\"MOE  F1 : {moe_f1_mean:.3f}\")\n",
    "    print(f\"FFN  F1 : {ffn_f1_mean:.3f}\")\n",
    "    print(f\"MOE  RL : {moe_rl_mean:.3f}\")\n",
    "    print(f\"FFN  RL : {ffn_rl_mean:.3f}\")\n",
    "\n",
    "    all_chunks.append(out_chunk)\n",
    "\n",
    "# 汇总与 CI\n",
    "_out_all = pd.concat(all_chunks, ignore_index=True) if all_chunks else pd.DataFrame(\n",
    "    columns=[\"F1_moe\",\"F1_ffn\",\"ROUGE-L_moe\",\"ROUGE-L_ffn\"]\n",
    ")\n",
    "\n",
    "moe_f1_mean, moe_f1_lo, moe_f1_hi = mean_ci(_out_all[\"F1_moe\"])\n",
    "ffn_f1_mean, ffn_f1_lo, ffn_f1_hi = mean_ci(_out_all[\"F1_ffn\"])\n",
    "moe_rl_mean, moe_rl_lo, moe_rl_hi = mean_ci(_out_all[\"ROUGE-L_moe\"])\n",
    "ffn_rl_mean, ffn_rl_lo, ffn_rl_hi = mean_ci(_out_all[\"ROUGE-L_ffn\"])\n",
    "\n",
    "diff_f1_mean, diff_f1_lo, diff_f1_hi = paired_diff_ci(_out_all[\"F1_moe\"], _out_all[\"F1_ffn\"])\n",
    "diff_rl_mean, diff_rl_lo, diff_rl_hi = paired_diff_ci(_out_all[\"ROUGE-L_moe\"], _out_all[\"ROUGE-L_ffn\"])\n",
    "\n",
    "print(\"\\n=== 95% CI (bootstrap, n={}) ===\".format(len(_out_all)))\n",
    "print(f\"MOE  F1 : {moe_f1_mean:.3f} [{moe_f1_lo:.3f}, {moe_f1_hi:.3f}]\")\n",
    "print(f\"FFN  F1 : {ffn_f1_mean:.3f} [{ffn_f1_lo:.3f}, {ffn_f1_hi:.3f}]\")\n",
    "print(f\"MOE  RL : {moe_rl_mean:.3f} [{moe_rl_lo:.3f}, {moe_rl_hi:.3f}]\")\n",
    "print(f\"FFN  RL : {ffn_rl_mean:.3f} [{ffn_rl_lo:.3f}, {ffn_rl_hi:.3f}]\")\n",
    "\n",
    "print(\"\\n=== Paired diff (MOE − FFN) 95% CI ===\")\n",
    "print(f\"ΔF1  : {diff_f1_mean:.3f} [{diff_f1_lo:.3f}, {diff_f1_hi:.3f}]\")\n",
    "print(f\"ΔRL  : {diff_rl_mean:.3f} [{diff_rl_lo:.3f}, {diff_rl_hi:.3f}]\")\n",
    "\n",
    "if not (diff_f1_lo <= 0.0 <= diff_f1_hi):\n",
    "    print(\"→ F1 差异在 95% CI 下显著。\")\n",
    "if not (diff_rl_lo <= 0.0 <= diff_rl_hi):\n",
    "    print(\"→ ROUGE-L 差异在 95% CI 下显著。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
